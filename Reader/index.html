<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Are you human?</title>
    <link rel="stylesheet" href="style.css">
</head>

<div class="details">
    Augmented Living Reader, edited & designed by Angelo Sarria, 2023
</div>

<header>Are you human?</header>

<body>

    <div class="text">
        Buried in the controversy this summer about Google's LaMDA language model, which an engineer claimed was sentient, is a hint about a big change that's come over artificial intelligence since Alan Turing defined the idea of the "Turing Test" in an essay in 1950.
    </div>

    <div class="text">
        Turing, a British mathematician who laid the groundwork for computing, offered what he called the "Imitation Game." Two entities, one a person, one a digital computer, are asked questions by a third entity, a human interrogator. The interrogator can't see the other two, and has to figure out simply from their type-written answers which of the two is human and which machine.
    </div>

    <div class="emoji">
        &#10070
    </div>

    <div class="quote">
        "I propose to consider the question, 'Can machines think?'"
    </div>

    <div class="quoteauthor">
        ~Alan Turing
    </div>

    <div class="emoji">
        &#10070
    </div>

    <div class="text">
        Why not, suggested Turing, let behavior settle the matter. If it answers like a human, then it can be credited with thinking.
    </div>

    <div class="text">
        Turing was sure that machines would get so good at the Turing Test that by the year 2000, "one will be able to speak of machines thinking without expecting to be contradicted."
    </div>

    <div class="text">
        A funny thing happened on the way to the future. Humans, it turns out, are spending more and more of their time inside of the world of machines, rather than the other way around.
    </div>

    <div class="text">
        Increasingly, humans spend their time doing stuff that a machine could do just as well if not better. One of the many achievements of modern software is to occupy people's time with easy tasks, such as the busy work you do on social media, things like posting, commenting, "liking," and Snapping. 
    </div>

    <div class="text">
        It's obvious that given half a chance, most machines could replicate social media behavior flawlessly. Not because programs such as OpenAI's GPT-3 language program are human-like, but because the low bar to interacting on social media has redefined what we might accept as "human" behavior.
    </div>

    <div class="emoji">
        &#10070
    </div>

    <div class="quote">
        If the average individual ascribes intelligence to AI, it is because the average individual spends more and more of their time engaging in online tasks that a machine could easily emulate.
    </div>

    <div class="emoji">
        &#10070
    </div>

    <div class="text">
        Everywhere you look, humans are increasingly engaged in behavior that would have seemed like science fiction just a couple decades ago.
    </div>
    
    <div class="text">
        Humans work around the clock to moderate content on platforms such as TikTok and Instagram Reels, an activity whose sheer volume of labor would have at one time seemed like workplace abuse, but that now is considered a basic necessity to maintain social media empires, and fend off regulators. 
    </div>

    <div class="text">
        Such activity could, again, conceivably be done as well or better by machine learning algorithms. 
    </div>

    <div class="emoji">
        &#10070
    </div>

    <div class="quote">
        Like John Henry and the steam engine, humans are increasingly trying to do a machine's job.
    </div>

    <div class="emoji">
        &#10070
    </div>

    <div class="text">
        In the case of Google's LaMDA chat bot, former Google engineer Blake Lemoine was assigned to test the program, an amusing echo of the Turing challenge. Only, in Lemoine's case, he was told up-front that it was a program. That did not prevent him from ascribing to LaMDA sentience, even a soul.
    </div>

    <div class="text">
        We don't know exactly how many hours, days, weeks or months Lemoine spent, but spending lots and lots of time chatting with something you've been told is a program is, again, a novel event in human history.
    </div>

    <div class="text">
        Computer scientist Hector Levesque has pointed out that "the Turing Test has a serious problem: it relies too much on deception." (Emphasis Levesque's.) The free-form nature of the test, writes Levesque, means an AI program can merely engage in a bag of tricks that feel human to the interrogator. 
    </div>

    <div class="text">
        Such programs "rely heavily on wordplay, jokes, quotations, asides, emotional outbursts, points of order, and so on," writes Levesque, "Everything, it would appear, except clear and direct answers to questions!"
    </div>

    <div class="text">
        The joke is on Levesque, however, and on all of us. Lemoine became captivated by that bag of tricks. Constant immersion in a world of screens, immersion to a degree that Turing never imagined, has made Turing's test no longer a test of machines but a test of humans, of what humans will accept as valid.
    </div>
    
    <div class="image">
        <img src="https://i.imgur.com/382wcSE.jpg" alt="I'm not a human" width="300" height="75">
    </div>
</body>
</html>